{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Talk to a PDF using LangChain and ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install pypdf\n",
    "# !pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/lucassoares/anaconda3/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 6745k  100 6745k    0     0   917k      0  0:00:07  0:00:07 --:--:-- 1107k\n"
     ]
    }
   ],
   "source": [
    "!curl -o paper.pdf https://arxiv.org/pdf/2303.13519.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import ChatVectorDBChain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning and Veriﬁcation of Task Structure in Instructional Videos\n",
      "Medhini Narasimhan1;2, Licheng Yu2, Sean Bell2, Ning Zhang2, Trevor Darrell1\n",
      "1UC Berkeley,2Meta AI\n",
      "https://medhini.github.io/task_structure\n",
      "Abstract\n",
      "Given the enormous number of instructional videos\n",
      "available online, learning a diverse array of multi-step task\n",
      "models from videos is an appealing goal. We introduce\n",
      "a new pre-trained video model, VideoTaskformer, focused\n",
      "on representing the semantics and structure of instructional\n",
      "videos. We pre-train VideoTaskformer using a simple and\n",
      "effective objective: predicting weakly supervised textual la-\n",
      "bels for steps that are randomly masked out from an instruc-\n",
      "tional video (masked step modeling). Compared to prior\n",
      "work which learns step representations locally, our ap-\n",
      "proach involves learning them globally, leveraging video of\n",
      "the entire surrounding task as context. From these learned\n",
      "representations, we can verify if an unseen video correctly\n",
      "executes a given task, as well as forecast which steps are\n",
      "likely to be taken after a given step. We introduce two new\n",
      "benchmarks for detecting mistakes in instructional videos,\n",
      "to verify if there is an anomalous step and if steps are exe-\n",
      "cuted in the right order. We also introduce a long-term fore-\n",
      "casting benchmark, where the goal is to predict long-range\n",
      "future steps from a given step. Our method outperforms pre-\n",
      "vious baselines on these tasks, and we believe the tasks will\n",
      "be a valuable way for the community to measure the quality\n",
      "of step representations. Additionally, we evaluate Video-\n",
      "Taskformer on 3 existing benchmarks—procedural activity\n",
      "recognition, step classiﬁcation, and step forecasting—and\n",
      "demonstrate on each that our method outperforms existing\n",
      "baselines and achieves new state-of-the-art performance.\n",
      "1. Introduction\n",
      "Picture this, you’re trying to build a bookshelf by watch-\n",
      "ing a YouTube video with several intricate steps. You’re\n",
      "annoyed by the need to repeatedly hit pause on the video\n",
      "and you’re unsure if you have gotten all the steps right so\n",
      "far. Fortunately, you have an interactive assistant that can\n",
      "guide you through the task at your own pace, verifying each\n",
      "\u0003Work done while an intern at Meta AI. Correspondence to\n",
      "medhini@berkeley.edu\n",
      "“Dip bread in batter”“Serve with maple syrup”\n",
      "EV1EV2EV3EV12\n",
      "Mask\n",
      "MaskVideoTaskformerT1T2T3T12Prediction over Step Classes\n",
      "“Dip bread in batter”EV1Prior work: Single clip step predictionOurs:Masked step prediction over all clips in video Figure 1: Prior work [13, 12] learns step representations from sin-\n",
      "gle short video clips, independent of the task, thus lacking knowl-\n",
      "edge of task structure. Our model, VideoTaskformer, learns step\n",
      "representations for masked video steps through the global context\n",
      "of all surrounding steps in the video, making our learned represen-\n",
      "tations aware of task semantics and structure.\n",
      "step as you perform it and interrupting you if you make a\n",
      "mistake. A composite task such as “ making a bookshelf ”\n",
      "involves multiple ﬁne-grained activities such as “ drilling\n",
      "holes ” and “ adding support blocks .” Accurately categoriz-\n",
      "ing these activities requires not only recognizing the indi-\n",
      "vidual steps that compose the task but also understanding\n",
      "the task structure, which includes the temporal ordering of\n",
      "the steps and multiple plausible ways of executing a step\n",
      "(e.g., one can beat eggs with a fork or a whisk). An ideal\n",
      "interactive assistant has both a high-level understanding of\n",
      "a broad range of tasks, as well as a low-level understanding\n",
      "of the intricate steps in the tasks, their temporal ordering,\n",
      "and the multiple ways of performing them.\n",
      "As seen in Fig. 1, prior work [12, 13] models step rep-\n",
      "resentations of a single step independent of the overall task\n",
      "context. This might not be the best strategy, given that steps\n",
      "for a task are related, and the way a step is situated in an\n",
      "overall task may contain important information about the\n",
      "step. To address this, we pre-train our model with a masked\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"./paper.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "pages = loader.load_and_split()\n",
    "print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "loaded in 22 embeddings\n",
      "loaded in 1 collections\n",
      "collection with name langchain already exists, returning existing collection\n",
      "Persisting DB to disk, putting it in the save folder .\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb = Chroma.from_documents(pages, embedding=embeddings,persist_directory=\".\")\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersistentDuckDB del, about to run persist\n",
      "Persisting DB to disk, putting it in the save folder .\n",
      "Answer:\n",
      "The VideoTaskformer is a method for learning step representations through masking in order to predict the correct steps for a given task based on a sequence of video clips.\n"
     ]
    }
   ],
   "source": [
    "pdf_qa = ChatVectorDBChain.from_llm(OpenAI(temperature=0.9, model_name=\"gpt-3.5-turbo\"),\n",
    "                                    vectordb, return_source_documents=True)\n",
    "\n",
    "query = \"What is the VideoTaskformer?\"\n",
    "result = pdf_qa({\"question\": query, \"chat_history\": \"\"})\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "explore",
   "language": "python",
   "name": "explore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
